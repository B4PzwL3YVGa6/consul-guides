#+title: Deploying Consul on Kubernetes
#+options: toc:nil title
#+property: header-args :export both :results output
* Deploying Consul on Kubernetes

This guide demonstrates deployment of [[http://consul.io][HashiCorp Consul]] running on Kubernetes
and deployed via the official HashiCorp [[https://helm.sh/][Helm]] Chart. The [[https://github.com/hashicorp/consul-helm][Chart]] can be used to deploy
both a set of Consul Servers and associated Consul Clients.

At the time of the writing of this guide, the Consul Helm Chart does not support
sophisticated multi-cluster or off-cluster (ex: VMs running elsewhere; Consul running on-cluster)
workloads. Stay tuned for updates for the more sophisticated multi-cluster and multi-datacenter
use-cases.

* Reference Materials

- [[https://consul.io/intro/index.html][Intro to key concepts in Consul]]
- [[https://kubernetes.io/docs/tutorials/kubernetes-basics/][Learn Kubernetes Basics]]

* Estimated Time to Complete
  - reading: 10 to 30 minutes
  - Consul Helm Chart-based deploy: < 5 minutes

* Personas
  - SRE/Operations or Ops-oriented Developers who:
    - are familiar with Kubernetes and Helm Charts
    - have need flexible and environment-agnostic:
      - Service Discovery
      - Health Checking
      - real-time config management
      - Service Mesh functionality with service-to-service mutual TLS and access control

* Challenge
  Deploy HashiCorp Consul running on a Kubernetes cluster using the [[https://github.com/hashicorp/consul-helm][official HashiCorp Helm Chart for Consul]].

* Prerequisites
  1. a running Kubernetes cluster
    - preferably with 3 or more "worker" nodes to allow for demonstraton of anti-affinity scheduling
    - k8s v 1.10 or later with the Downward API and status.hostIP referenceable as an environment variable
  1. [[https://kubernetes.io/docs/tasks/tools/install-kubectl/][kubectl]] v 1.10 or later binary installed
  1. [[https://docs.helm.sh/using_helm/][Helm]] binaries installed
  1. [[https://helm.sh][Helm Tiller]] installed on the Kubernetes cluster

* Solution
** Steps
*** Validate access to a Kubernetes cluster
    This guide assumes the reader has access to a working Kubernetes cluster. Preferably a cluster with multiple "worker"
    nodes in order to allow for familiariazation with the Chart's embedded affinity/anti-affinity scheduling features.

    The [[http://kubernetes.io][Kubernetes website]] has lots of information about how to deploy a Kubernetes cluster or you might consider
    provisioning a test cluster with Terraform code provided by Hashicorp:

    - bundled with the official Consul Helm Chat [[https://github.com/hashicorp/consul-helm/tree/master/test/terraform][here]]
    - via the HashiCorp Solutions Engineering team [[https://github.com/hashicorp/terraform-guides/tree/master/infrastructure-as-code][here]].

    First let's use kubectl to validate we have a configured connection to a k8s cluster:

    #+begin_src sh
    kubectl cluster-info
    #+end_src

    #+RESULTS:
    : Kubernetes master is running at https://35.197.4.209
    : GLBCDefaultBackend is running at https://35.197.4.209/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
    : Heapster is running at https://35.197.4.209/api/v1/namespaces/kube-system/services/heapster/proxy
    : KubeDNS is running at https://35.197.4.209/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    : Metrics-server is running at https://35.197.4.209/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
    : 
    : To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

    #+begin_src sh
    kubectl version
    #+end_src

    #+RESULTS:
    : Client Version: version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.2", GitCommit:"bb9ffb1654d4a729bb4cec18ff088eacc153c239", GitTreeState:"clean", BuildDate:"2018-08-07T23:17:28Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/amd64"}
    : Server Version: version.Info{Major:"1", Minor:"10+", GitVersion:"v1.10.6-gke.2", GitCommit:"384b4eaa132ca9a295fcb3e5dfc74062b257e7df", GitTreeState:"clean", BuildDate:"2018-08-15T00:10:14Z", GoVersion:"go1.9.3b4", Compiler:"gc", Platform:"linux/amd64"}

*** Validating Helm's Tiller on the Kubernetes cluster
    This guide also assumes that Helm's [[https://docs.helm.sh/glossary/#tiller][Tiller]] is installed and functioning properly on the k8s cluster. The [[https://helm.sh][Helm website]]
    has documentation on how to install and configure Helm and Helm Tiller.

    To validate a working Tiller (assumes the default of Tiller running in the Kubernets 'kube-system' namespace):

    #+begin_src sh
    kubectl get pods --namespace=kube-system | grep tiller
    #+end_src

    #+RESULTS:
    : tiller-deploy-895d57dd9-q82bp                                   1/1       Running   0          14d

    #+begin_src sh
    helm version
    #+end_src

    #+RESULTS:
    : Client: &version.Version{SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"}
    : Server: &version.Version{SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"}

    Your versions my differ slightly but so long as you have Running Tiller and the Client and Server versions are
    reporing reasonable values you are probably in good shape.

*** Download and install the official HashiCorp Consul Helm Chart
    There are a number of ways to install a Helm Chart but the process changes slightly from version to version of Helm
    and the workflow is different during Helm Chart development versus production usage. To keep things simple,
    we'll simply clone the Consul chart repo locally and drive the process from there. This is more similar to
    a development workflow but it lets us understand the workings of the Consul Helm Chart without diving too deep into
    Helm Chat workflow.

    #+begin_src sh :export both :results output :prologue rm -rf consul
      # FIXME: change this once the repo is public available
      # git clone https://github.com/hashicorp/consul-helm ./consul-helm
      git clone --progress git@github.com:hashicorp/consul-helm ./consul
    #+end_src

    #+RESULTS:

*** Deploy (default config) Consul on Kubernetes
    We will use the downloaded Consul Helm Chart to deploy a Consul cluster. Note specification of
    the 'ui.service.type' parameter to the Consul Helm Chart. Specifying 'ui.service.type' allows the Operator
    to expose the Consul UI service for use outside of the Kubernetes cluster and thus makes the Consul UI
    available from a web browser.

    **Note**: The correct 'ui.service.type' will be highly dependent on how you your Kubernetes networking is configured.
    'LoadBalancer' works fine for default Google Container Engine-deployed Kubernetes through the default integration
    with Google Cloud's external load balancer. For other environments, 'Ingress' or 'NodePort' might make more sense.

    **Another Note**: We disable the [[https://www.consul.io/intro/getting-started/connect.html][Consul Connect]] service mesh integration because that's a topic for another
    guide. Again, stay tuned.

    #+begin_src sh -i :prologue helm delete --purge $(helm list --deployed | grep consul | cut -f1)
    helm install ./consul --set ui.service.type=LoadBalancer --set connectInject.enabled=false \
      --set server.connect=false --wait
    #+end_src

    #+RESULTS:
    #+begin_example
    release "vetoed-lionfish" deleted
    NAME:   idolized-chipmunk
    LAST DEPLOYED: Fri Sep  7 11:30:08 2018
    NAMESPACE: default
    STATUS: DEPLOYED

    RESOURCES:
    ==> v1/ConfigMap
    NAME                                    DATA  AGE
    idolized-chipmunk-consul-server-config  1     58s
    idolized-chipmunk-consul-tests          1     58s

    ==> v1/Service
    NAME                             TYPE          CLUSTER-IP    EXTERNAL-IP    PORT(S)                                                         AGE
    idolized-chipmunk-consul-server  ClusterIP     None          <none>         8500/TCP,8301/TCP,8301/UDP,8302/TCP,8302/UDP,8300/TCP,8600/TCP  58s
    idolized-chipmunk-consul-ui      LoadBalancer  10.15.248.92  35.230.110.93  80:32684/TCP                                                    58s

    ==> v1/DaemonSet
    NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
    idolized-chipmunk-consul  5        5        5      5           5          <none>         58s

    ==> v1/StatefulSet
    NAME                             DESIRED  CURRENT  AGE
    idolized-chipmunk-consul-server  3        3        58s

    ==> v1beta1/PodDisruptionBudget
    NAME                             MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE
    idolized-chipmunk-consul-server  N/A            0                0                    58s

    ==> v1/Pod(related)
    NAME                               READY  STATUS   RESTARTS  AGE
    idolized-chipmunk-consul-4tqv2     1/1    Running  0         58s
    idolized-chipmunk-consul-dxnhg     1/1    Running  0         58s
    idolized-chipmunk-consul-j9zmh     1/1    Running  0         58s
    idolized-chipmunk-consul-nhltr     1/1    Running  0         58s
    idolized-chipmunk-consul-pbp5d     1/1    Running  0         58s
    idolized-chipmunk-consul-server-0  1/1    Running  0         58s
    idolized-chipmunk-consul-server-1  1/1    Running  0         58s
    idolized-chipmunk-consul-server-2  1/1    Running  0         58s


    #+end_example

    There are a number of things going on above, some of which require prerequisite knowledge of Kubernetes, but let's
    step through the basics:

    - The Chart deploys an on-cluster Consul cluster using a [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][Headless Service]] with a [[https://kubernetes.io/docs/concepts/services-networking/service/][ClusterIP]]. This means, besides the Consul UI, Consul ports are exposed only in the Pod networking space.
    - Consul Servers are deployed as a [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/][StatefulSet]] which is helpful for starting/upgrading the cluster in an orderly fashion and binding [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/][PersistentVolumeClaims]] to the resulting Consul Server Pods.
    - Consul Clients are deployed to each Kubernetes node as a [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/][DaemonSet]].

    The following visualization is representative of what one might see for a small but near-production-grade k8s
    cluster and may be helpful for understanding the Consul Helm Chart:

    #+caption: Consul Helm Chart deploy of Consul Server and Consul Client Pods
    #+attr_html: :alt Consul Helm Chart deploy of Consul Server and Consul Client Pods
    [[./static/Consul-on-k8s.png]]

*** Inspect the resulting Consul cluster
    You can query the Kuberenes cluster to validate the information above via the following commands:

    #+begin_src sh
    helm list
    #+end_src

    #+RESULTS:
    : NAME             	REVISION	UPDATED                 	STATUS  	CHART       	APP VERSION	NAMESPACE
    : idolized-chipmunk	1       	Fri Sep  7 11:30:08 2018	DEPLOYED	consul-0.1.0	           	default  

    #+begin_src sh
    kubectl get services
    #+end_src

    #+RESULTS:
    : NAME                              TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                                                          AGE
    : idolized-chipmunk-consul-server   ClusterIP      None           <none>          8500/TCP,8301/TCP,8301/UDP,8302/TCP,8302/UDP,8300/TCP,8600/TCP   59s
    : idolized-chipmunk-consul-ui       LoadBalancer   10.15.248.92   35.230.110.93   80:32684/TCP                                                     59s
    : kubernetes                        ClusterIP      10.15.240.1    <none>          443/TCP                                                          15d

    #+begin_src sh
    kubectl get statefulset
    #+end_src

    #+RESULTS:
    : NAME                              DESIRED   CURRENT   AGE
    : idolized-chipmunk-consul-server   3         3         1m

    #+begin_src sh
    kubectl get daemonset
    #+end_src

    #+RESULTS:
    : NAME                       DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
    : idolized-chipmunk-consul   5         5         5         5            5           <none>          1m

*** Accessing the Consul UI

    On Linux:
    #+begin_example
    xdg-open http://<k8s service LB address>:80
    #+end_example

    On OSX:
    #+begin_example
    open http://<k8s service LB address>:80
    #+end_example

*** Register a test pod against Consul service discovery
    Now that the Consul cluster has been deployed, let's deploy a couple of workloads which demonstrate how Pods might
    leverage Consul's service discovery functionality.

    Here's a simple Pod definition we'll feed to kubectl to demonstrate that a container/Pod can talk to the Consul
    client running on its underlying k8s node by querying the node's internal IP via [[https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/][Downward API]] 'status.hostIP'
    as an environment variable.

    The node's internal IP is saved in the environment variable 'K8S_HOST_IP'. This is then interplated into the Pods
    entrypoint to perform a Consul server 'peer' query against via the 'httpie' command-line utility.

    #+begin_src sh
    cat clusterdebug.yml
    #+end_src

    #+RESULTS:
    #+begin_example
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: clusterdebug
    spec:
      restartPolicy: Never
      containers:
	- name: clusterdebug
	  image: nrvale0/clusterdebug:latest # large but useful-for-debugging image
	  imagePullPolicy: Always
	  command: [ "/bin/sh", "-c" ]
	  args:
	  - http get http://${K8S_HOST_IP}:8500/v1/status/peers --verbose
	  env:
	    - name: K8S_HOST_IP
	      valueFrom:
		fieldRef:
		  fieldPath: status.hostIP # get the node IP/Consul client IP from the Downward API
    #+end_example

    The Pod will execute the httpie command, exit, and we can view the results via the Pod's stored logs:

    #+begin_src sh :prologue kubectl delete -f clusterdebug.yml > /dev/null 2>&1 && sleep 2
    kubectl apply -f clusterdebug.yml --wait
    #+end_src

    #+RESULTS:
    : pod/clusterdebug created

    #+begin_src sh :proloug
    kubectl logs clusterdebug
    #+end_src

    #+RESULTS:

    That's all well and good but next let's spawn a Pod which actually performs a Consul service discovery registration:

    #+begin_src sh
    cat nginx.yml
    #+end_src

    #+RESULTS:
    #+begin_example
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: consul-reg
    data:

      regjson: |-
	{
	  "Name": "nginx",
	  "Address": "{{K8S_POD_IP}}",
	  "Port": 80
	}

      dockerrun: |-
	#!/bin/bash

	set -u

	function hl() {
	  printf "\n\n%s\n\n" '--------------------------------------------------------------------------------'
	}

	echo "Our Pod IP is ${K8S_POD_IP}..."
	cat /tmp/templates/reg.json.template | sed -e "s/{{K8S_POD_IP}}/${K8S_POD_IP}/g" > /tmp/consul/reg.json
	hl

	echo "Register the service..."
	(set -x; http --verbose put "http://${K8S_HOST_IP}:8500/v1/agent/service/register" < /tmp/consul/reg.json)
	hl

	echo "Check that the nginx service exists in the catalog..."
	(set -x; http --verbose get "http://${K8S_HOST_IP}:8500/v1/catalog/services")
	hl

	echo "Finally, check that our service contains the node..."
	(set -x; http --json get "http://${K8S_HOST_IP}:8500/v1/catalog/service/nginx" | jq .[].ServiceAddress)
	hl

	sleep infinity # sleep forever in case we wan to kubectl exec into this container...

    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      restartPolicy: Never
      containers:

	- name: nginx
	  image: nginx:latest

	  volumeMounts:
	  - name: consul-reg-templates
	    mountPath: /tmp/templates
	  - name: consul-reg
	    mountPath: /tmp/consul
	  - name: dockerrun
	    mountPath: /tmp/docker-run

	- name: consul-registration
	  image: nrvale0/clusterdebug
	  imagePullPolicy: Always
	  env:
	    - name: K8S_HOST_IP
	      valueFrom:
		fieldRef:
		  fieldPath: status.hostIP # get the node IP/Consul client IP from the Downward API
	    - name: K8S_POD_IP
	      valueFrom:
		fieldRef:
		  fieldPath: status.podIP # get the pod IP the Downward API

	  volumeMounts:
	  - name: consul-reg-templates
	    mountPath: /tmp/templates
	  - name: consul-reg
	    mountPath: /tmp/consul
	  - name: dockerrun
	    mountPath: /tmp/docker-run

	  command: [ "/bin/bash", "-c" ]
	  args: [ "cp /tmp/templates/docker-run.sh /tmp/docker-run/docker-run.sh && chmod +x /tmp/docker-run/docker-run.sh && stdbuf -i0 -o0 -e0 /tmp/docker-run/docker-run.sh" ]

      volumes:

      - name: consul-reg-templates
	configMap:
	  name: consul-reg
	  items:
	    - key: regjson
	      path: reg.json.template
	    - key: dockerrun
	      path: docker-run.sh

      - name: consul-reg
	emptyDir: {}

      - name: dockerrun
	emptyDir: {}
    #+end_example

    #+begin_src sh :prologue kubectl delete -f nginx.yml > /dev/null 2>&1 && sleep 2
    kubectl apply -f nginx.yml --wait
    #+end_src

    #+RESULTS:
    : configmap/consul-reg created
    : pod/nginx created

    We can check the log output of the sidecar responsible for executing theh service registration to validate the
    registration:

    #+begin_src sh :prologue sleep 10
    kubectl logs nginx -c consul-registration
    #+end_src

    #+RESULTS:
    #+begin_example
    Our Pod IP is 10.12.2.235...
    + http --verbose put http://10.138.0.2:8500/v1/agent/service/register


    --------------------------------------------------------------------------------

    Register the service...
    PUT /v1/agent/service/register HTTP/1.1
    User-Agent: HTTPie/0.9.4
    Accept-Encoding: gzip, deflate
    Accept: application/json
    Connection: keep-alive
    Content-Type: application/json
    Content-Length: 63
    Host: 10.138.0.2:8500
    
    {
      "Name": "nginx",
      "Address": "10.12.2.235",
      "Port": 80
    }

    HTTP/1.1 200 OK
    Vary: Accept-Encoding
    Date: Fri, 07 Sep 2018 18:32:32 GMT
    Content-Length: 0
    


    --------------------------------------------------------------------------------

    Check that the nginx service exists in the catalog...
    + http --verbose get http://10.138.0.2:8500/v1/catalog/services
    GET /v1/catalog/services HTTP/1.1
    User-Agent: HTTPie/0.9.4
    Accept-Encoding: gzip, deflate
    Accept: */*
    Connection: keep-alive
    Host: 10.138.0.2:8500
    


    HTTP/1.1 200 OK
    Content-Encoding: gzip
    Content-Type: application/json
    Vary: Accept-Encoding
    X-Consul-Effective-Consistency: leader
    X-Consul-Index: 42
    X-Consul-Knownleader: true
    X-Consul-Lastcontact: 0
    Date: Fri, 07 Sep 2018 18:32:32 GMT
    Content-Length: 46
    
    {"consul":[],"nginx":[]}

    --------------------------------------------------------------------------------

    Finally, check that our service contains the node...
    + jq '.[].ServiceAddress'
    + http --json get http://10.138.0.2:8500/v1/catalog/service/nginx
    "10.12.2.235"


    --------------------------------------------------------------------------------

    #+end_example

    Or we can check the Consul UI itself:

    #+caption: Consul UI showing Service Discovery registration of the nginx Pod
    #+attr_html: :alt Consul UI showing Service Discovery registration of the nginx Pod
    [[./static/Consul-UI-service-nginx.png]]

    The Pod name and Pod IP in the above screenshot will differ from that referenced in elsewhere in this guide.

*** Clean Up (optional)
    If your are done experimenting with Consul on Kubernetes and would like to tear down and clean up:

    #+begin_example
    helm delete --purge <Helm install name>
    kubectl delete -f nginx.yml
    kubectl delete -f clusterdebug.yml
    #+end_example

    Note that the Consul pods are backed by [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims][PersistentVolumeClaims]] which, by design, require manual cleanup:

    #+begin_example
    kubectl get pvc
    for i in <list of PVCs> ; do kubectl delete pvc $i; done
    #+end_example

* Additional References
  - FIXME: put some follow-up references here
